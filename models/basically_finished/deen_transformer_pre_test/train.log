2023-05-14 18:40:52,771 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2023-05-14 18:40:52,771 - INFO - joeynmt.helpers -                           cfg.name : deen_transformer_pre_test
2023-05-14 18:40:52,771 - INFO - joeynmt.helpers -                cfg.joeynmt_version : 2.0.0
2023-05-14 18:40:52,771 - INFO - joeynmt.helpers -                     cfg.data.train : data/train_test
2023-05-14 18:40:52,771 - INFO - joeynmt.helpers -                       cfg.data.dev : data/dev
2023-05-14 18:40:52,772 - INFO - joeynmt.helpers -                      cfg.data.test : data/test
2023-05-14 18:40:52,772 - INFO - joeynmt.helpers -              cfg.data.dataset_type : plain
2023-05-14 18:40:52,772 - INFO - joeynmt.helpers -                  cfg.data.src.lang : de
2023-05-14 18:40:52,772 - INFO - joeynmt.helpers -             cfg.data.src.lowercase : False
2023-05-14 18:40:52,772 - INFO - joeynmt.helpers -                 cfg.data.src.level : bpe
2023-05-14 18:40:52,772 - INFO - joeynmt.helpers -             cfg.data.src.voc_limit : 32000
2023-05-14 18:40:52,772 - INFO - joeynmt.helpers -          cfg.data.src.voc_min_freq : 1
2023-05-14 18:40:52,772 - INFO - joeynmt.helpers -            cfg.data.src.max_length : 100
2023-05-14 18:40:52,772 - INFO - joeynmt.helpers -              cfg.data.src.voc_file : shared_models/joint-vocab.txt
2023-05-14 18:40:52,772 - INFO - joeynmt.helpers -        cfg.data.src.tokenizer_type : subword-nmt
2023-05-14 18:40:52,772 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.pretokenizer : none
2023-05-14 18:40:52,772 - INFO - joeynmt.helpers - cfg.data.src.tokenizer_cfg.num_merges : 3200
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -   cfg.data.src.tokenizer_cfg.codes : data/codes3200.bpe
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -                  cfg.data.trg.lang : en
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -             cfg.data.trg.lowercase : False
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -                 cfg.data.trg.level : bpe
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -             cfg.data.trg.voc_limit : 32000
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -          cfg.data.trg.voc_min_freq : 1
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -            cfg.data.trg.max_length : 100
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -              cfg.data.trg.voc_file : shared_models/joint-vocab.txt
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -        cfg.data.trg.tokenizer_type : subword-nmt
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.pretokenizer : none
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers - cfg.data.trg.tokenizer_cfg.num_merges : 3200
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -   cfg.data.trg.tokenizer_cfg.codes : data/codes3200.bpe
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -              cfg.testing.beam_size : 5
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -             cfg.testing.beam_alpha : 1.0
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -           cfg.training.random_seed : 42
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -             cfg.training.optimizer : adam
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -         cfg.training.normalization : tokens
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -         cfg.training.learning_rate : 0.0003
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -            cfg.training.batch_size : 2048
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -            cfg.training.batch_type : token
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -       cfg.training.eval_batch_size : 1024
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -       cfg.training.eval_batch_type : token
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -            cfg.training.scheduling : plateau
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -              cfg.training.patience : 8
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -          cfg.training.weight_decay : 0.0
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -       cfg.training.decrease_factor : 0.7
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -                cfg.training.epochs : 10
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -       cfg.training.validation_freq : 500
2023-05-14 18:40:52,773 - INFO - joeynmt.helpers -          cfg.training.logging_freq : 100
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -           cfg.training.eval_metric : ['bleu']
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -             cfg.training.model_dir : models/deen_transformer_pre_test
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -             cfg.training.overwrite : False
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -               cfg.training.shuffle : True
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -              cfg.training.use_cuda : True
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -     cfg.training.max_output_length : 100
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -     cfg.training.print_valid_sents : [0, 1, 2, 3, 4]
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -       cfg.training.label_smoothing : 0.3
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -              cfg.model.initializer : xavier_uniform
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -         cfg.model.bias_initializer : zeros
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -                cfg.model.init_gain : 1.0
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -        cfg.model.embed_initializer : xavier_uniform
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -          cfg.model.embed_init_gain : 1.0
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -          cfg.model.tied_embeddings : True
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -             cfg.model.tied_softmax : True
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -             cfg.model.encoder.type : transformer
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -       cfg.model.encoder.num_layers : 4
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -        cfg.model.encoder.num_heads : 2
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 256
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -      cfg.model.encoder.hidden_size : 256
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -          cfg.model.encoder.ff_size : 512
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -          cfg.model.encoder.dropout : 0
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -             cfg.model.decoder.type : transformer
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -       cfg.model.decoder.num_layers : 1
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -        cfg.model.decoder.num_heads : 2
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 256
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0
2023-05-14 18:40:52,774 - INFO - joeynmt.helpers -      cfg.model.decoder.hidden_size : 256
2023-05-14 18:40:52,775 - INFO - joeynmt.helpers -          cfg.model.decoder.ff_size : 512
2023-05-14 18:40:52,775 - INFO - joeynmt.helpers -          cfg.model.decoder.dropout : 0
2023-05-14 18:40:52,780 - INFO - joeynmt.data - Building tokenizer...
2023-05-14 18:40:52,787 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2023-05-14 18:40:52,787 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2023-05-14 18:40:52,787 - INFO - joeynmt.data - Loading train set...
2023-05-14 18:40:52,790 - INFO - joeynmt.data - Building vocabulary...
2023-05-14 18:40:52,906 - INFO - joeynmt.data - Loading dev set...
2023-05-14 18:40:52,908 - INFO - joeynmt.data - Loading test set...
2023-05-14 18:40:52,912 - INFO - joeynmt.data - Data loaded.
2023-05-14 18:40:52,912 - INFO - joeynmt.data - Train dataset: PlaintextDataset(split=train, len=999, src_lang=de, trg_lang=en, has_trg=True, random_subset=-1)
2023-05-14 18:40:52,913 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=500, src_lang=de, trg_lang=en, has_trg=True, random_subset=-1)
2023-05-14 18:40:52,913 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=2999, src_lang=de, trg_lang=en, has_trg=True, random_subset=-1)
2023-05-14 18:40:52,914 - INFO - joeynmt.data - First training example:
	[SRC] gem@@ ä@@ ß der vom Europäischen Parlament und von der gesam@@ ten Europäischen Union n@@ un@@ mehr ständi@@ g ver@@ tre@@ ten@@ en Lin@@ ie möchte ich Sie jedoch bit@@ ten , den gan@@ zen Ein@@ f@@ lu@@ ß Ih@@ res Am@@ tes und der Institu@@ tion , die Sie ver@@ treten , bei dem Prä@@ sident@@ schaf@@ ts@@ kan@@ di@@ d@@ aten und G@@ ou@@ ver@@ ne@@ ur von Tex@@ as , Ge@@ or@@ ge W@@ . B@@ us@@ h , der zur Aus@@ setzung der V@@ oll@@ stre@@ ck@@ ung des To@@ des@@ ur@@ teil@@ s und zur Be@@ gn@@ a@@ di@@ gung des Ver@@ ur@@ teil@@ ten be@@ fu@@ gt ist , gel@@ ten@@ d zu machen .
	[TRG] however , I would ask you , in acc@@ ord@@ ance with the line which is now con@@ st@@ ant@@ ly fol@@ low@@ ed by the European Parliament and by the whole of the European Community , to make represent@@ ations , using the wei@@ ght of your pres@@ ti@@ gi@@ ous off@@ ice and the institu@@ tion you re@@ present , to the President and to the Go@@ vern@@ or of Tex@@ as , Mr B@@ us@@ h , who has the power to order a stay of ex@@ ec@@ ution and to re@@ pri@@ e@@ ve the con@@ dem@@ ned per@@ son .
2023-05-14 18:40:52,914 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) die (9) der
2023-05-14 18:40:52,914 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) die (9) der
2023-05-14 18:40:52,914 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 4117
2023-05-14 18:40:52,914 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 4117
2023-05-14 18:40:52,957 - INFO - joeynmt.model - Building an encoder-decoder model...
2023-05-14 18:40:53,045 - INFO - joeynmt.model - Enc-dec model built.
2023-05-14 18:40:53,055 - INFO - joeynmt.model - Total params: 3953152
2023-05-14 18:40:53,055 - DEBUG - joeynmt.model - Trainable parameters: ['decoder.layers.0.dec_layer_norm.bias', 'decoder.layers.0.dec_layer_norm.weight', 'decoder.layers.0.feed_forward.layer_norm.bias', 'decoder.layers.0.feed_forward.layer_norm.weight', 'decoder.layers.0.feed_forward.pwff_layer.0.bias', 'decoder.layers.0.feed_forward.pwff_layer.0.weight', 'decoder.layers.0.feed_forward.pwff_layer.3.bias', 'decoder.layers.0.feed_forward.pwff_layer.3.weight', 'decoder.layers.0.src_trg_att.k_layer.bias', 'decoder.layers.0.src_trg_att.k_layer.weight', 'decoder.layers.0.src_trg_att.output_layer.bias', 'decoder.layers.0.src_trg_att.output_layer.weight', 'decoder.layers.0.src_trg_att.q_layer.bias', 'decoder.layers.0.src_trg_att.q_layer.weight', 'decoder.layers.0.src_trg_att.v_layer.bias', 'decoder.layers.0.src_trg_att.v_layer.weight', 'decoder.layers.0.trg_trg_att.k_layer.bias', 'decoder.layers.0.trg_trg_att.k_layer.weight', 'decoder.layers.0.trg_trg_att.output_layer.bias', 'decoder.layers.0.trg_trg_att.output_layer.weight', 'decoder.layers.0.trg_trg_att.q_layer.bias', 'decoder.layers.0.trg_trg_att.q_layer.weight', 'decoder.layers.0.trg_trg_att.v_layer.bias', 'decoder.layers.0.trg_trg_att.v_layer.weight', 'decoder.layers.0.x_layer_norm.bias', 'decoder.layers.0.x_layer_norm.weight', 'encoder.layers.0.feed_forward.layer_norm.bias', 'encoder.layers.0.feed_forward.layer_norm.weight', 'encoder.layers.0.feed_forward.pwff_layer.0.bias', 'encoder.layers.0.feed_forward.pwff_layer.0.weight', 'encoder.layers.0.feed_forward.pwff_layer.3.bias', 'encoder.layers.0.feed_forward.pwff_layer.3.weight', 'encoder.layers.0.layer_norm.bias', 'encoder.layers.0.layer_norm.weight', 'encoder.layers.0.src_src_att.k_layer.bias', 'encoder.layers.0.src_src_att.k_layer.weight', 'encoder.layers.0.src_src_att.output_layer.bias', 'encoder.layers.0.src_src_att.output_layer.weight', 'encoder.layers.0.src_src_att.q_layer.bias', 'encoder.layers.0.src_src_att.q_layer.weight', 'encoder.layers.0.src_src_att.v_layer.bias', 'encoder.layers.0.src_src_att.v_layer.weight', 'encoder.layers.1.feed_forward.layer_norm.bias', 'encoder.layers.1.feed_forward.layer_norm.weight', 'encoder.layers.1.feed_forward.pwff_layer.0.bias', 'encoder.layers.1.feed_forward.pwff_layer.0.weight', 'encoder.layers.1.feed_forward.pwff_layer.3.bias', 'encoder.layers.1.feed_forward.pwff_layer.3.weight', 'encoder.layers.1.layer_norm.bias', 'encoder.layers.1.layer_norm.weight', 'encoder.layers.1.src_src_att.k_layer.bias', 'encoder.layers.1.src_src_att.k_layer.weight', 'encoder.layers.1.src_src_att.output_layer.bias', 'encoder.layers.1.src_src_att.output_layer.weight', 'encoder.layers.1.src_src_att.q_layer.bias', 'encoder.layers.1.src_src_att.q_layer.weight', 'encoder.layers.1.src_src_att.v_layer.bias', 'encoder.layers.1.src_src_att.v_layer.weight', 'encoder.layers.2.feed_forward.layer_norm.bias', 'encoder.layers.2.feed_forward.layer_norm.weight', 'encoder.layers.2.feed_forward.pwff_layer.0.bias', 'encoder.layers.2.feed_forward.pwff_layer.0.weight', 'encoder.layers.2.feed_forward.pwff_layer.3.bias', 'encoder.layers.2.feed_forward.pwff_layer.3.weight', 'encoder.layers.2.layer_norm.bias', 'encoder.layers.2.layer_norm.weight', 'encoder.layers.2.src_src_att.k_layer.bias', 'encoder.layers.2.src_src_att.k_layer.weight', 'encoder.layers.2.src_src_att.output_layer.bias', 'encoder.layers.2.src_src_att.output_layer.weight', 'encoder.layers.2.src_src_att.q_layer.bias', 'encoder.layers.2.src_src_att.q_layer.weight', 'encoder.layers.2.src_src_att.v_layer.bias', 'encoder.layers.2.src_src_att.v_layer.weight', 'encoder.layers.3.feed_forward.layer_norm.bias', 'encoder.layers.3.feed_forward.layer_norm.weight', 'encoder.layers.3.feed_forward.pwff_layer.0.bias', 'encoder.layers.3.feed_forward.pwff_layer.0.weight', 'encoder.layers.3.feed_forward.pwff_layer.3.bias', 'encoder.layers.3.feed_forward.pwff_layer.3.weight', 'encoder.layers.3.layer_norm.bias', 'encoder.layers.3.layer_norm.weight', 'encoder.layers.3.src_src_att.k_layer.bias', 'encoder.layers.3.src_src_att.k_layer.weight', 'encoder.layers.3.src_src_att.output_layer.bias', 'encoder.layers.3.src_src_att.output_layer.weight', 'encoder.layers.3.src_src_att.q_layer.bias', 'encoder.layers.3.src_src_att.q_layer.weight', 'encoder.layers.3.src_src_att.v_layer.bias', 'encoder.layers.3.src_src_att.v_layer.weight', 'src_embed.lut.weight']
2023-05-14 18:40:53,056 - INFO - joeynmt.training - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="post", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=4117),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4117),
	loss_function=XentLoss(criterion=KLDivLoss(), smoothing=0.3))
2023-05-14 18:40:53,056 - INFO - joeynmt.builders - Adam(lr=0.0003, weight_decay=0.0, betas=(0.9, 0.999))
2023-05-14 18:40:53,056 - INFO - joeynmt.builders - ReduceLROnPlateau(mode=min, verbose=False, threshold_mode=abs, eps=0.0, factor=0.7, patience=8)
2023-05-14 18:40:53,056 - INFO - joeynmt.training - Train stats:
	device: cpu
	n_gpu: 0
	16-bits training: False
	gradient accumulation: 1
	batch size per device: 2048
	effective batch size (w. parallel & accumulation): 2048
2023-05-14 18:40:53,056 - INFO - joeynmt.training - EPOCH 1
2023-05-14 18:41:37,267 - INFO - joeynmt.training - Epoch   1: total training loss 188.00
2023-05-14 18:41:37,268 - INFO - joeynmt.training - EPOCH 2
2023-05-14 18:42:16,627 - INFO - joeynmt.training - Epoch   2: total training loss 173.16
2023-05-14 18:42:16,627 - INFO - joeynmt.training - EPOCH 3
2023-05-14 18:42:32,466 - INFO - joeynmt.training - Epoch   3, Step:      100, Batch Loss:     4.047584, Batch Acc: 0.080940, Tokens per Sec:      941, Lr: 0.000300
2023-05-14 18:42:55,925 - INFO - joeynmt.training - Epoch   3: total training loss 166.99
2023-05-14 18:42:55,925 - INFO - joeynmt.training - EPOCH 4
2023-05-14 18:43:37,358 - INFO - joeynmt.training - Epoch   4: total training loss 169.02
2023-05-14 18:43:37,359 - INFO - joeynmt.training - EPOCH 5
2023-05-14 18:44:10,792 - INFO - joeynmt.training - Epoch   5, Step:      200, Batch Loss:     3.968812, Batch Acc: 0.095653, Tokens per Sec:      913, Lr: 0.000300
2023-05-14 18:44:18,347 - INFO - joeynmt.training - Epoch   5: total training loss 166.16
2023-05-14 18:44:18,347 - INFO - joeynmt.training - EPOCH 6
2023-05-14 18:44:57,131 - INFO - joeynmt.training - Epoch   6: total training loss 162.88
2023-05-14 18:44:57,132 - INFO - joeynmt.training - EPOCH 7
2023-05-14 18:45:37,591 - INFO - joeynmt.training - Epoch   7: total training loss 157.92
2023-05-14 18:45:37,591 - INFO - joeynmt.training - EPOCH 8
2023-05-14 18:45:44,511 - INFO - joeynmt.training - Epoch   8, Step:      300, Batch Loss:     3.750892, Batch Acc: 0.111562, Tokens per Sec:      998, Lr: 0.000300
2023-05-14 18:46:17,823 - INFO - joeynmt.training - Epoch   8: total training loss 153.26
2023-05-14 18:46:17,824 - INFO - joeynmt.training - EPOCH 9
2023-05-14 18:46:57,466 - INFO - joeynmt.training - Epoch   9: total training loss 148.44
2023-05-14 18:46:57,466 - INFO - joeynmt.training - EPOCH 10
2023-05-14 18:47:20,207 - INFO - joeynmt.training - Epoch  10, Step:      400, Batch Loss:     3.454279, Batch Acc: 0.151248, Tokens per Sec:      943, Lr: 0.000300
2023-05-14 18:47:36,557 - INFO - joeynmt.training - Epoch  10: total training loss 140.22
2023-05-14 18:47:36,558 - INFO - joeynmt.training - Training ended after  10 epochs.
2023-05-14 18:47:36,558 - INFO - joeynmt.training - Best validation result (greedy) at step        0:    inf ppl.
2023-05-14 18:47:36,608 - INFO - joeynmt.model - Building an encoder-decoder model...
2023-05-14 18:47:36,733 - INFO - joeynmt.model - Enc-dec model built.
